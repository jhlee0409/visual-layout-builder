/**
 * AI Model Strategies Test Script
 *
 * ê° AI ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„± ì „ëµì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
 *
 * ì‹¤í–‰ ë°©ë²•:
 * ```bash
 * npx tsx scripts/test-ai-model-strategies.ts
 * ```
 */

import {
  createPromptStrategy,
  getModelRecommendations,
  getAvailableModelIds,
  getModelsByCategory,
} from "../lib/prompt-strategies"
import type { AIModelId, PromptGenerationOptions } from "../types/ai-models"
import type { LaydlerSchema } from "../types/schema"
import { sampleSchemas } from "../lib/sample-data"

// ANSI color codes
const colors = {
  reset: "\x1b[0m",
  bright: "\x1b[1m",
  red: "\x1b[31m",
  green: "\x1b[32m",
  yellow: "\x1b[33m",
  blue: "\x1b[34m",
  magenta: "\x1b[35m",
  cyan: "\x1b[36m",
}

function log(message: string, color: keyof typeof colors = "reset") {
  console.log(`${colors[color]}${message}${colors.reset}`)
}

function section(title: string) {
  log(`\n${"=".repeat(80)}`, "bright")
  log(title, "bright")
  log("=".repeat(80), "bright")
}

function subsection(title: string) {
  log(`\n${"-".repeat(80)}`, "cyan")
  log(title, "cyan")
  log("-".repeat(80), "cyan")
}

/**
 * Test 1: Factory ê¸°ë³¸ ë™ì‘ í…ŒìŠ¤íŠ¸
 */
function testFactoryBasics() {
  section("Test 1: Factory ê¸°ë³¸ ë™ì‘ í…ŒìŠ¤íŠ¸")

  try {
    // ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
    const availableModels = getAvailableModelIds()
    log(`âœ“ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ê°œìˆ˜: ${availableModels.length}`, "green")
    log(`  ëª¨ë¸: ${availableModels.slice(0, 5).join(", ")} ...`, "blue")

    // ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸
    const modelsByCategory = getModelsByCategory()
    log(`\nâœ“ ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸:`, "green")
    for (const [category, models] of Object.entries(modelsByCategory)) {
      if (models.length > 0) {
        log(`  - ${category}: ${models.length}ê°œ`, "blue")
      }
    }

    // ì „ëµ ìƒì„± í…ŒìŠ¤íŠ¸
    const testModels: AIModelId[] = [
      "claude-sonnet-4.5",
      "gpt-4.1",
      "gemini-2.5-pro",
      "deepseek-r1",
    ]

    log(`\nâœ“ ì „ëµ ìƒì„± í…ŒìŠ¤íŠ¸:`, "green")
    for (const modelId of testModels) {
      const strategy = createPromptStrategy(modelId)
      log(`  - ${modelId}: ${strategy.metadata.name} (${strategy.metadata.provider})`, "blue")
    }

    log(`\nâœ… Factory ê¸°ë³¸ ë™ì‘ í…ŒìŠ¤íŠ¸ í†µê³¼`, "green")
    return true
  } catch (error) {
    log(`\nâŒ Factory ê¸°ë³¸ ë™ì‘ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ${error}`, "red")
    return false
  }
}

/**
 * Test 2: ëª¨ë¸ ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
 */
function testModelRecommendation() {
  section("Test 2: ëª¨ë¸ ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")

  try {
    // ì‹œë‚˜ë¦¬ì˜¤ 1: ê°„ë‹¨í•œ í”„ë¡œì íŠ¸, ë¹„ìš© ë¯¼ê°
    subsection("ì‹œë‚˜ë¦¬ì˜¤ 1: ê°„ë‹¨í•œ í”„ë¡œì íŠ¸, ë¹„ìš© ë¯¼ê°")
    const recommendations1 = getModelRecommendations({
      schemaComplexity: "simple",
      responsiveComplexity: "simple",
      needsFrameworkSpecialization: false,
      costSensitivity: "high",
      qualityRequirement: "draft",
      speedPriority: "high",
    })

    log(`ì¶”ì²œ ëª¨ë¸ (Top 3):`, "cyan")
    recommendations1.slice(0, 3).forEach((rec, index) => {
      log(
        `  ${index + 1}. ${rec.modelId} (ì ìˆ˜: ${rec.score}, ë¹„ìš©: ${rec.estimatedCost}, í’ˆì§ˆ: ${rec.estimatedQuality})`,
        "blue"
      )
      log(`     ì´ìœ : ${rec.reason}`, "blue")
    })

    // ì‹œë‚˜ë¦¬ì˜¤ 2: ë³µì¡í•œ í”„ë¡œì íŠ¸, í”„ë¡œë•ì…˜ í’ˆì§ˆ
    subsection("ì‹œë‚˜ë¦¬ì˜¤ 2: ë³µì¡í•œ í”„ë¡œì íŠ¸, í”„ë¡œë•ì…˜ í’ˆì§ˆ")
    const recommendations2 = getModelRecommendations({
      schemaComplexity: "complex",
      responsiveComplexity: "complex",
      needsFrameworkSpecialization: true,
      costSensitivity: "low",
      qualityRequirement: "production",
      speedPriority: "medium",
    })

    log(`ì¶”ì²œ ëª¨ë¸ (Top 3):`, "cyan")
    recommendations2.slice(0, 3).forEach((rec, index) => {
      log(
        `  ${index + 1}. ${rec.modelId} (ì ìˆ˜: ${rec.score}, ë¹„ìš©: ${rec.estimatedCost}, í’ˆì§ˆ: ${rec.estimatedQuality})`,
        "blue"
      )
      log(`     ì´ìœ : ${rec.reason}`, "blue")
    })

    // ì‹œë‚˜ë¦¬ì˜¤ 3: ì¤‘ê°„ ë³µì¡ë„, ê· í˜• ì¡íŒ ìš”êµ¬ì‚¬í•­
    subsection("ì‹œë‚˜ë¦¬ì˜¤ 3: ì¤‘ê°„ ë³µì¡ë„, ê· í˜• ì¡íŒ ìš”êµ¬ì‚¬í•­")
    const recommendations3 = getModelRecommendations({
      schemaComplexity: "medium",
      responsiveComplexity: "medium",
      needsFrameworkSpecialization: true,
      costSensitivity: "medium",
      qualityRequirement: "production",
      speedPriority: "medium",
    })

    log(`ì¶”ì²œ ëª¨ë¸ (Top 3):`, "cyan")
    recommendations3.slice(0, 3).forEach((rec, index) => {
      log(
        `  ${index + 1}. ${rec.modelId} (ì ìˆ˜: ${rec.score}, ë¹„ìš©: ${rec.estimatedCost}, í’ˆì§ˆ: ${rec.estimatedQuality})`,
        "blue"
      )
      log(`     ì´ìœ : ${rec.reason}`, "blue")
    })

    log(`\nâœ… ëª¨ë¸ ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ í†µê³¼`, "green")
    return true
  } catch (error) {
    log(`\nâŒ ëª¨ë¸ ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ${error}`, "red")
    return false
  }
}

/**
 * Test 3: í”„ë¡¬í”„íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸ (ê° ëª¨ë¸ë³„)
 */
function testPromptGeneration() {
  section("Test 3: í”„ë¡¬í”„íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸")

  try {
    // Sample schema ë¡œë“œ
    const schema = sampleSchemas.github

    if (!schema) {
      throw new Error("Sample schemaë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    }

    log(`ìƒ˜í”Œ Schema ë¡œë“œ ì™„ë£Œ: ${schema.components.length}ê°œ ì»´í¬ë„ŒíŠ¸`, "cyan")

    // í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ë° ì˜µì…˜
    const testCases: Array<{
      modelId: AIModelId
      options: PromptGenerationOptions
      description: string
    }> = [
      {
        modelId: "claude-sonnet-4.5",
        options: {
          targetModel: "claude-sonnet-4.5",
          optimizationLevel: "quality",
          verbosity: "detailed",
          chainOfThought: true,
        },
        description: "Claude Sonnet 4.5 (Quality, CoT)",
      },
      {
        modelId: "gpt-4.1",
        options: {
          targetModel: "gpt-4.1",
          optimizationLevel: "balanced",
          verbosity: "normal",
          includeExamples: true,
        },
        description: "GPT-4.1 (Balanced, Examples)",
      },
      {
        modelId: "gemini-2.5-pro",
        options: {
          targetModel: "gemini-2.5-pro",
          optimizationLevel: "balanced",
          verbosity: "normal",
        },
        description: "Gemini 2.5 Pro (Framework-focused)",
      },
      {
        modelId: "deepseek-r1",
        options: {
          targetModel: "deepseek-r1",
          optimizationLevel: "quick",
          verbosity: "minimal",
          costSensitive: true,
        },
        description: "DeepSeek R1 (Cost-efficient)",
      },
    ]

    for (const testCase of testCases) {
      subsection(testCase.description)

      const strategy = createPromptStrategy(testCase.modelId)
      const result = strategy.generatePrompt(schema, "react", "tailwind", testCase.options)

      if (result.success && result.prompt) {
        log(`âœ“ í”„ë¡¬í”„íŠ¸ ìƒì„± ì„±ê³µ`, "green")
        log(`  - í† í° ì¶”ì •: ${result.estimatedTokens?.toLocaleString() || "N/A"}`, "blue")
        log(`  - ì„¹ì…˜ ìˆ˜: ${result.sections?.length || "N/A"}`, "blue")
        log(
          `  - í”„ë¡¬í”„íŠ¸ ê¸¸ì´: ${result.prompt.length.toLocaleString()} characters`,
          "blue"
        )
        log(`  - ëª¨ë¸: ${result.modelId}`, "blue")

        if (result.warnings && result.warnings.length > 0) {
          log(`  âš  ê²½ê³  ${result.warnings.length}ê°œ:`, "yellow")
          result.warnings.slice(0, 3).forEach((warning) => {
            log(`    - ${warning}`, "yellow")
          })
        }

        // í”„ë¡¬í”„íŠ¸ ìƒ˜í”Œ ì¶œë ¥ (ì²« 500ì)
        log(`\n  í”„ë¡¬í”„íŠ¸ ìƒ˜í”Œ (ì²« 500ì):`, "magenta")
        log(`  "${result.prompt.substring(0, 500)}..."`, "blue")
      } else {
        log(`âœ— í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨`, "red")
        if (result.errors) {
          result.errors.forEach((error) => {
            log(`  - ${error}`, "red")
          })
        }
        return false
      }
    }

    log(`\nâœ… í”„ë¡¬í”„íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸ í†µê³¼`, "green")
    return true
  } catch (error) {
    log(`\nâŒ í”„ë¡¬í”„íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ${error}`, "red")
    console.error(error)
    return false
  }
}

/**
 * Test 4: í”„ë¡¬í”„íŠ¸ ì°¨ì´ì  ë¹„êµ (ëª¨ë¸ë³„)
 */
function testPromptDifferences() {
  section("Test 4: í”„ë¡¬í”„íŠ¸ ì°¨ì´ì  ë¹„êµ (ëª¨ë¸ë³„)")

  try {
    const schema = sampleSchemas.github

    if (!schema) {
      throw new Error("Sample schemaë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    }

    const models: AIModelId[] = ["claude-sonnet-4.5", "gpt-4.1", "gemini-2.5-pro", "deepseek-r1"]

    log(`ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ íŠ¹ì„± ë¹„êµ:`, "cyan")
    log(`\n${"Model".padEnd(25)} | ${"Tokens".padEnd(10)} | ${"Length".padEnd(10)} | ${"Verbosity"}`)
    log("-".repeat(80))

    const results: Array<{
      modelId: AIModelId
      tokens: number
      length: number
      verbosity: string
    }> = []

    for (const modelId of models) {
      const strategy = createPromptStrategy(modelId)
      const result = strategy.generatePrompt(schema, "react", "tailwind", {
        targetModel: modelId,
        verbosity: "normal",
      })

      if (result.success && result.prompt) {
        const tokens = result.estimatedTokens || 0
        const length = result.prompt.length
        const verbosity =
          length < 3000 ? "Minimal" : length < 6000 ? "Normal" : "Detailed"

        results.push({ modelId, tokens, length, verbosity })

        log(
          `${modelId.padEnd(25)} | ${tokens.toLocaleString().padEnd(10)} | ${length.toLocaleString().padEnd(10)} | ${verbosity}`,
          "blue"
        )
      }
    }

    // í†µê³„
    log(`\ní†µê³„:`, "cyan")
    const avgTokens = results.reduce((sum, r) => sum + r.tokens, 0) / results.length
    const avgLength = results.reduce((sum, r) => sum + r.length, 0) / results.length
    log(`  - í‰ê·  í† í°: ${Math.round(avgTokens).toLocaleString()}`, "blue")
    log(`  - í‰ê·  ê¸¸ì´: ${Math.round(avgLength).toLocaleString()} characters`, "blue")

    const shortest = results.reduce((min, r) => (r.length < min.length ? r : min))
    const longest = results.reduce((max, r) => (r.length > max.length ? r : max))
    log(`  - ê°€ì¥ ì§§ì€ í”„ë¡¬í”„íŠ¸: ${shortest.modelId} (${shortest.length.toLocaleString()})`, "blue")
    log(`  - ê°€ì¥ ê¸´ í”„ë¡¬í”„íŠ¸: ${longest.modelId} (${longest.length.toLocaleString()})`, "blue")

    log(`\nâœ… í”„ë¡¬í”„íŠ¸ ì°¨ì´ì  ë¹„êµ í…ŒìŠ¤íŠ¸ í†µê³¼`, "green")
    return true
  } catch (error) {
    log(`\nâŒ í”„ë¡¬í”„íŠ¸ ì°¨ì´ì  ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ${error}`, "red")
    return false
  }
}

/**
 * Main test runner
 */
async function main() {
  log("\n", "reset")
  log("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—", "bright")
  log("â•‘         AI Model Strategies Test Suite                                    â•‘", "bright")
  log("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•", "bright")

  const tests = [
    { name: "Factory ê¸°ë³¸ ë™ì‘", fn: testFactoryBasics },
    { name: "ëª¨ë¸ ì¶”ì²œ ì‹œìŠ¤í…œ", fn: testModelRecommendation },
    { name: "í”„ë¡¬í”„íŠ¸ ìƒì„±", fn: testPromptGeneration },
    { name: "í”„ë¡¬í”„íŠ¸ ì°¨ì´ì  ë¹„êµ", fn: testPromptDifferences },
  ]

  let passedTests = 0
  let failedTests = 0

  for (const test of tests) {
    const passed = test.fn()
    if (passed) {
      passedTests++
    } else {
      failedTests++
    }
  }

  // Summary
  section("Test Summary")
  log(`Total Tests: ${tests.length}`, "cyan")
  log(`Passed: ${passedTests}`, "green")
  log(`Failed: ${failedTests}`, failedTests > 0 ? "red" : "green")
  log(`Success Rate: ${((passedTests / tests.length) * 100).toFixed(1)}%`, "bright")

  if (failedTests === 0) {
    log(`\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!`, "green")
    process.exit(0)
  } else {
    log(`\nâŒ ${failedTests}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨`, "red")
    process.exit(1)
  }
}

// Run tests
main().catch((error) => {
  log(`\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: ${error}`, "red")
  console.error(error)
  process.exit(1)
})
